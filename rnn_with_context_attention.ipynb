{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UChicagoCompLx at the 2018 SMM4H Shared Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "numpy.random.seed(1)\n",
    "\n",
    "import keras\n",
    "import nn_modules\n",
    "import utilities\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = '1'\n",
    "path_to_files = f'../smm4h_shared_task{TASK}/task{TASK}'\n",
    "CV = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pandas.read_csv(\n",
    "    filepath_or_buffer=path_to_files + 'train.tsv', \n",
    "    sep='\\t', \n",
    "    engine='c')\n",
    "\n",
    "val = pandas.read_csv(\n",
    "    filepath_or_buffer=path_to_files + 'val.tsv', \n",
    "    sep='\\t', \n",
    "    engine='c')\n",
    "\n",
    "if CV: \n",
    "    train = pandas.concat([train,val]).sample(frac=1)\n",
    "print(f'Training set examples: {train.shape[0]}')\n",
    "\n",
    "if CV:\n",
    "    print('Training for submission/cross-validation; will not use withheld validation set...')\n",
    "else:\n",
    "    print(f'Validation examples: {val.shape[0]}')\n",
    "    \n",
    "test = pandas.read_csv(\n",
    "    filepath_or_buffer=path_to_files + 'test.tsv', \n",
    "    sep='\\t', \n",
    "    engine='c')\n",
    "print(f'Test set examples: {test.shape[0]}')\n",
    "\n",
    "# for TASK 2, shift labels by -1\n",
    "if TASK == '2':\n",
    "    train.target = train.target - 1\n",
    "    val.target = val.target - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tweet = train.tweet.apply(utilities.text_preprocessor)\n",
    "test.tweet = test.tweet.apply(utilities.text_preprocessor)\n",
    "if not (CV or FINAL):\n",
    "    val.tweet = val.tweet.apply(utilities.text_preprocessor)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_per_doc = 50\n",
    "max_words = 7000\n",
    "\n",
    "train_strings, val_strings, test_strings = train.tweet, val.tweet, test.tweet\n",
    "corpus = numpy.concatenate((train_strings, val_strings, test_strings),axis=0)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(train_strings), \n",
    "    maxlen=max_words_per_doc)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(val_strings), \n",
    "    maxlen=max_words_per_doc)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(test_strings), \n",
    "    maxlen=max_words_per_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "embeddings_file = f'../embeddings/datastories.twitter.{dim}d.txt'\n",
    "\n",
    "embedding_matrix = utilities.load_embeddings(\n",
    "    embeddings_file=embeddings_file,\n",
    "    word_index=tokenizer.word_index, \n",
    "    max_words=max_words, \n",
    "    embedding_dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels from train and val sets\n",
    "train_labels = train.target.values\n",
    "val_labels = val.target.values\n",
    "class_weights = utilities.get_class_weights(train_labels) \n",
    "\n",
    "if TASK == '2':\n",
    "    train_labels = keras.utils.to_categorical(train_labels)\n",
    "    val_labels = keras.utils.to_categorical(val_labels)\n",
    "    \n",
    "if TASK == '4':\n",
    "    test_labels = test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing & compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_with_context_att():\n",
    "    input_text = keras.layers.Input(shape=(max_words_per_doc,), dtype='int32')\n",
    "\n",
    "    # embedding\n",
    "    embedding_params = {\n",
    "        'gaussian_noise' : 0.3,\n",
    "        'embedding_do' : 0.3\n",
    "    }\n",
    "\n",
    "    emb_text = nn_modules.embedding(\n",
    "        input_text=input_text,\n",
    "        max_sequence_length=max_words_per_doc,\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        **embedding_params)\n",
    "\n",
    "    encoder_params = {\n",
    "        'rnn_layers' : 2,\n",
    "        'linear_do': 0.3,\n",
    "        'recurrent_do': 0.3,\n",
    "        'attention_do': 0.5,\n",
    "    }\n",
    "\n",
    "    # encoding\n",
    "    representation = nn_modules.rnn_encoders_with_attention(\n",
    "        nb_cells=150,\n",
    "        embeddings=emb_text,\n",
    "        **encoder_params)\n",
    "\n",
    "    softmax_params = {'l2_dense' : 0.0001}\n",
    "\n",
    "    # prediction\n",
    "    output_probs = nn_modules.softmax_classifier(\n",
    "        representation,\n",
    "        nb_classes=3 if TASK=='2' else 1,\n",
    "        **softmax_params)\n",
    "\n",
    "    # instantiate and compile\n",
    "    model = keras.models.Model(inputs=input_text,outputs=output_probs)\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001,clipnorm=1)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy' if TASK=='2' else 'binary_crossentropy',\n",
    "        metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "min_delta = 0.001\n",
    "patience_epochs = 5\n",
    "\n",
    "if CV:\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1)\n",
    "    \n",
    "    f1_folds = []\n",
    "    precision_folds = []\n",
    "    recall_folds = []\n",
    "        \n",
    "    for i,(train_idx,val_idx) in enumerate(skf.split(x_train,train_labels)):\n",
    "    \n",
    "        earlystopper = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_acc', \n",
    "            min_delta=min_delta, \n",
    "            patience=patience_epochs,\n",
    "            verbose=0, \n",
    "            mode='max')\n",
    "        callbacks = [earlystopper]\n",
    "        model = lstm_with_context_att()\n",
    "\n",
    "        print(f'=====Training model in fold: {i+1}=====')\n",
    "        history = model.fit(\n",
    "            x=x_train[train_idx], \n",
    "            y=train_labels[train_idx],\n",
    "            validation_data=(x_train[val_idx],train_labels[val_idx]),\n",
    "            epochs=50, \n",
    "            batch_size=50,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights)\n",
    "        \n",
    "        if TASK == '2':\n",
    "            preds_val = numpy.argmax(model.predict(x_train[val_idx]), axis=1) + 1\n",
    "        else:\n",
    "            preds_val = [round(x[0]) for x in model.predict(x_train[val_idx])]\n",
    "            \n",
    "        f1 = f1_score(\n",
    "            y_true=train_labels[val_idx] + (1 if TASK=='2' else 0), \n",
    "            y_pred=preds_val, \n",
    "            average='micro' if TASK=='2' else 'binary', \n",
    "            labels=[1,2])\n",
    "        precision = precision_score(\n",
    "            y_true=train_labels[val_idx] + (1 if TASK=='2' else 0), \n",
    "            y_pred=preds_val,\n",
    "            average='micro' if TASK=='2' else 'binary', \n",
    "            labels=[1,2])\n",
    "        recall = recall_score(\n",
    "            y_true=train_labels[val_idx] + (1 if TASK=='2' else 0), \n",
    "            y_pred=preds_val,\n",
    "            average='micro' if TASK=='2' else 'binary', \n",
    "            labels=[1,2])\n",
    "        \n",
    "        f1_folds.append(f1)\n",
    "        precision_folds.append(precision)\n",
    "        recall_folds.append(recall)\n",
    "        \n",
    "        keras.backend.clear_session()\n",
    "        del model\n",
    "\n",
    "    print(f'average precision_score: {numpy.mean(precision_folds)}')\n",
    "    print(f'average recall_score: {numpy.mean(recall_folds)}')\n",
    "    print(f'average f1_score: {numpy.mean(f1_folds)}')  \n",
    "\n",
    "else:\n",
    "\n",
    "    earlystopper = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc', \n",
    "        min_delta=min_delta, \n",
    "        patience=patience_epochs,\n",
    "        verbose=0, \n",
    "        mode='max')\n",
    "    callbacks = [earlystopper]\n",
    "    \n",
    "    model = lstm_with_context_att()\n",
    "    history = model.fit(\n",
    "        x=x_train, \n",
    "        y=train_labels,\n",
    "        validation_data=(x_val,val_labels),\n",
    "        epochs=50, \n",
    "        batch_size=50,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights)\n",
    "    \n",
    "    if TASK == '2':\n",
    "        preds_val = numpy.argmax(model.predict(x_val), axis=1) + 1\n",
    "    else:\n",
    "        preds_val = [round(x[0]) for x in model.predict(x_val)]\n",
    "    f1 = f1_score(\n",
    "        y_true=val.target + (1 if TASK=='2' else 0), \n",
    "        y_pred=preds_val, \n",
    "        average='micro' if TASK=='2' else 'binary', \n",
    "        labels=[1,2])\n",
    "    print(f'f1_score: {f1}')\n",
    "    precision = precision_score(\n",
    "        y_true=val.target + (1 if TASK=='2' else 0), \n",
    "        y_pred=preds_val,\n",
    "        average='micro' if TASK=='2' else 'binary', \n",
    "        labels=[1,2])\n",
    "    print(f'precision_score: {precision}')\n",
    "    recall = recall_score(\n",
    "        y_true=val.target + (1 if TASK=='2' else 0), \n",
    "        y_pred=preds_val,\n",
    "        average='micro' if TASK=='2' else 'binary', \n",
    "        labels=[1,2])\n",
    "    print(f'recall_score: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION = False\n",
    "if SUBMISSION and not CV:\n",
    "    os.makedirs('../models/',exist_ok=True)\n",
    "    os.makedirs(f'../models/task{TASK}',exist_ok=True)\n",
    "    model.save(f'../models/task{TASK}/lstm_with_context_attn.h5')\n",
    "    \n",
    "    os.makedirs('../submission/',exist_ok=True)\n",
    "    os.makedirs(f'../submission/task{TASK}',exist_ok=True)\n",
    "    if TASK == '2':\n",
    "        preds = numpy.argmax(model.predict(x_test), axis=1) + 1\n",
    "    else:\n",
    "        preds = [int(round(x[0])) for x in model.predict(x_test)]\n",
    "    \n",
    "    save_to_path = f'../submission/task{TASK}/'\n",
    "    timestamp = datetime.datetime.now().isoformat('_', timespec='minutes')\n",
    "    fname = f'task{TASK}_' + 'lstm_with_context_attn' + ('_final' if FINAL else '') + '.tsv'\n",
    "    \n",
    "    submission = pandas.DataFrame({'tweet_id': test.tweet_id.values, 'target':preds})\n",
    "    submission.target = submission.target.astype(int)\n",
    "    submission.to_csv(\n",
    "        path_or_buf=save_to_path + fname, \n",
    "        index=False, \n",
    "        header=None, \n",
    "        sep='\\t', \n",
    "        columns = ['tweet_id','target'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
